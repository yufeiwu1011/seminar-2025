{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c2f3ab-f31c-496d-a3e0-4e1bd704db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os     \n",
    "if \"KERAS_BACKEND\" not in os.environ:\n",
    "    #os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  \n",
    "    os.environ[\"KERAS_BACKEND\"] = \"jax\"  \n",
    "import bayesflow as bf\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "from numba import njit\n",
    "import math\n",
    "import ipynbname\n",
    "from pathlib import Path\n",
    "RNG = np.random.default_rng(2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4a7e91-c89b-4d59-8e3a-7bc3ca7f975d",
   "metadata": {},
   "source": [
    "In this exercise, you need to load networks trained with standard DDM data, and networks trained with contaminated DDM data. Then you test the robustness of these two networks and do a small comparison.\n",
    "\n",
    "As we want to load the trained models, it is important to know where your notebook and models (.keras) are stored. The following code helps \n",
    "you to locate them. If you already know your path, you can define it manually as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ccc42c8c-dca0-4f5c-9776-1284a501122a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\System32\\bayesflow\n"
     ]
    }
   ],
   "source": [
    "#notebook_path = your_path\n",
    "notebook_path = Path(ipynbname.path()).resolve().parent\n",
    "print(notebook_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8832bdb6-86e9-4652-b33b-f24a72088e26",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05a197b-d04b-412b-986d-225d52baf5fe",
   "metadata": {},
   "source": [
    "This tutorial focuses on:\n",
    "1. The simulation and inference of stochastic cognitive models in BayesFlow\n",
    "2. Testing and improving the robustness of inference in BayesFlow\n",
    "\n",
    "Here we fit Drift Diffusion Model (DDM), a popular stochastic model in decision-making field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeb1c5c-4bad-458a-b43f-b9239f58e2c6",
   "metadata": {},
   "source": [
    "### Drift Diffusion Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2e5edf-b823-47f9-bffd-f7b127c31796",
   "metadata": {},
   "source": [
    "Below is a graphical illustration of the drift diffusion process and the resulting reaction time data in a hypothetical visual recognition memory task. Participants view an image and judge whether it is old (i.e., previously seen) or new (i.e., previously unseen). The rows in the resulting data table correspond to individual trials of the experiment, with conditions and responses coded as 0 (old image) and 1 (new image)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebb36d4-6c56-409e-95d1-5d67b87b494b",
   "metadata": {},
   "source": [
    "![Diffusion Model Plot](plots/Diffusion_Model_plot.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51344515-4631-45e9-843a-d8a06c15f131",
   "metadata": {},
   "source": [
    "The standard DDM assume that the decision-making process is a evidence accumulation process, after the accumulated evidence reaches a certain boundary, a decision is made. The upper and lower boundaries corresponds to different choices. It thus has four types of parameters:\n",
    "1. Drift rate ($v$): the average rate of evidence accumulation under certain condtion\n",
    "2. Boundary separation ($a$): the distance between two boundaries\n",
    "3. Response bias ($z$): the relative starting point of the diffusion process\n",
    "4. Non-decision time ($ndt$): any time unrelated to the decision process itself (e.g., stimulus encoding, motor execution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e16312b-2f49-43d9-8d69-ff4c36d1c85f",
   "metadata": {},
   "source": [
    "### Sensitivity to Outliers\n",
    "DDM is sensitive to outliers due to the nature of its assumptions. The key parameter non-decision time ($ndt$) is estimated to be lower than the shortest reaction time in the data set by design. Since the decision process is jointly determined by all DDM parameters, when a short outlier is present, it can distort not only the estimate of $T_{er}$ but also those of other parameters, leading to biased results Consequently, addressing the influence of outliers has been a persistent challenge in DDM fitting. We thus use DDM as an example to test the robustness of amortized Bayesian inference, and try to improve the robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726ba239-ed26-4039-ace8-d5119d7cfe49",
   "metadata": {},
   "source": [
    "## A Standard DDM Estimator\n",
    "We first train a standard DDM estimator, that is, to simulate standard reaction time data from DDM to train neural networks.\n",
    "### Single trial simulation\n",
    "Here we simulate one single diffusion trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7c456f1f-0c97-4ac2-b237-c2e185e25093",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True, cache=True)\n",
    "def diffusion_trial(v, a, z, ndt, dt=1e-3, max_steps=15000):\n",
    "    \"\"\"Simulates a trial from the diffusion model.\"\"\"\n",
    "    n_steps = 0\n",
    "    x = a * z\n",
    "    mu = v * dt\n",
    "    sigma = math.sqrt(dt)\n",
    "    \n",
    "    # Simulate a single DM path\n",
    "    for n_steps in range(max_steps):\n",
    "        # DDM equation\n",
    "        x += mu + sigma * np.random.normal(0,1)\n",
    "        # Stop when out of bounds\n",
    "        if x <= 0.0 or x >= a:\n",
    "            break\n",
    "    \n",
    "    rt = float(n_steps) * dt\n",
    "    \n",
    "    if x > 0:\n",
    "        resp = 1.\n",
    "    else:\n",
    "        resp = 0.\n",
    "    return rt+ndt,resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e28cbfa-5292-49a1-857f-49ff928c4b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_trial(v = 1, a = 2, z = 0.5, ndt = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a857f3e5-d32e-4782-89c4-fccfbf985c32",
   "metadata": {},
   "source": [
    "### Prior\n",
    "We define a prior that is wide enough to cover a realistic range of true parameters. Two drift rates are assumed, corresponding to two conditions in the experimental design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc4bf82-0fa7-47fc-9ff0-ea85782c2f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffusion_prior():\n",
    "    \"Generates a random draw from the joint prior distribution.\"\n",
    "    #normal distribution for the drift rates\n",
    "    v_1 = RNG.uniform(-7,7)\n",
    "    v_2 = RNG.uniform(-7,7)\n",
    "    a = RNG.uniform(0.5,5)\n",
    "    ndt = RNG.gamma(1.5, 1 / 5.0)\n",
    "    z = RNG.uniform(.01,.99)\n",
    "    return dict(v = np.array((v_1, v_2)),\n",
    "                a = a,\n",
    "                ndt = ndt,\n",
    "                z = z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547dfdc0-e359-4f8e-a839-db6ccf7187b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_prior()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88e4d60-7aed-475f-9603-e8745fe9a8bc",
   "metadata": {},
   "source": [
    "### Number of Observations\n",
    "The number of observations (trials) in each batch is randomly sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd53136-aec6-40f6-bf17-140e8e2be80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#design matrix\n",
    "min_obs = 100\n",
    "max_obs = 500\n",
    "def meta(batch_size, num_obs = None):\n",
    "\n",
    "    if num_obs == None:\n",
    "        num_obs = np.random.randint(min_obs, max_obs)\n",
    "    return dict(num_obs = num_obs)\n",
    "    #return dict(num_obs = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2bb7bf-beee-467a-8860-79ff3a35eac9",
   "metadata": {},
   "source": [
    "### Observational Model\n",
    "We wrap the prior, single trial simulator, and number of observations up to obtain a complete model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae9d0e6-d230-46fc-9584-6cd107218913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffusion_experiment(v, a, z, ndt, num_obs, rng=None, *args):\n",
    "    out = np.zeros((num_obs, 2))\n",
    "\n",
    "    #create an array with condition (dummy variable, with two values \"0\" and \"1\")\n",
    "    num_conditions = 2\n",
    "    counts = np.random.multinomial(num_obs, [1/num_conditions] * num_conditions)\n",
    "    condition = np.concatenate([np.full(count, i) for i, count in enumerate(counts)])\n",
    "    np.random.shuffle(condition)\n",
    "    \n",
    "    for n in range(num_obs):\n",
    "        index = condition[n]\n",
    "        rt,resp = diffusion_trial(v[index], a, z, ndt)\n",
    "        out[n, :] = np.array([rt,resp])\n",
    "\n",
    "    #log transform the reaction time for faster convergence\n",
    "    out[:,0] = np.log(out[:,0])\n",
    "    \n",
    "    return dict(rt = out[:,0], resp = out[:,1], con = condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefd4ec5-c9d6-4c93-aece-3f9ac0004339",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = bf.simulators.make_simulator([diffusion_prior, diffusion_experiment], meta_fn=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541bf55b-762e-4458-87ce-96b932b74219",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data = simulator.sample(200)\n",
    "print(\"Number of observations:\", sim_data[\"num_obs\"])\n",
    "print(\"Shape of rt:\", sim_data[\"rt\"].shape)\n",
    "print(\"Shape of response:\", sim_data[\"resp\"].shape)\n",
    "print(\"Shape of condition:\", sim_data[\"con\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dce18b-c833-41af-bb13-f2ed03c2d9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "par_keys = [\"v\", \"a\", \"ndt\", \"z\"]\n",
    "par_names = [r\"v_1\", r\"v_2\", \"a\", \"ndt\", \"z\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cbfa56-d0bc-4ca8-ab51-cea0f1c62e95",
   "metadata": {},
   "source": [
    "### Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faee9cc4-7f9b-4f4a-a7a8-4912da3d9ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter = (\n",
    "    bf.Adapter()\n",
    "    .broadcast(\"num_obs\", to=\"rt\")\n",
    "    .concatenate([\"v\", \"a\",\"ndt\",\"z\"], into = \"inference_variables\")\n",
    "    .as_set([\"rt\", \"resp\", \"con\"])\n",
    "    .concatenate([\"rt\",\"resp\", \"con\"], into = \"summary_variables\")\n",
    "    .rename(\"num_obs\", \"inference_conditions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3eafef-bd04-4e33-af89-a692003c7376",
   "metadata": {},
   "source": [
    "### Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71c6e56-78f3-42a8-ae1c-6ecb7acc730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_network = bf.networks.SetTransformer(summary_dim=10)\n",
    "inference_network = bf.networks.CouplingFlow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c726f16-433e-4780-8875-17bcdfce4a7f",
   "metadata": {},
   "source": [
    "If you want to train a new model, you can define ```checkpoint_filepath =``` and ```checkpoint_name =``` to store and name your trained model in the ```BasicWorkflow``` object. They are now commented out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be29fcc-0d11-4464-8fb9-af0fbc195b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = bf.BasicWorkflow(\n",
    "    simulator=simulator,\n",
    "    adapter=adapter,\n",
    "    inference_network=inference_network,\n",
    "    summary_network=summary_network,\n",
    "    initial_learning_rate=5e-4,\n",
    "    optimizer = optimizer,\n",
    "    summary_variables = [\"rt\",\"resp\", \"con\"],\n",
    "    inference_variables = [\"drifts\",\"threshold\",\"ndt\",\"z\"],\n",
    "    inference_conditions = [\"num_obs\"],\n",
    "    #checkpoint_filepath = checkpoint_path,\n",
    "    #checkpoint_name = \"standard_ddm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67972e5-80c3-43b4-9341-5ee2f5b73adb",
   "metadata": {},
   "source": [
    "As we already have the trained model, we just need to load it with ```keras.saving.load_model```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68c54f0-dbb3-4699-a49f-0c0911df61e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the path where you store the keras\n",
    "checkpoint_path_standard = notebook_path/\"standard_ddm.keras\"\n",
    "workflow.approximator = keras.saving.load_model(checkpoint_path_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654c2987-5657-4a81-a268-0acdda32f2af",
   "metadata": {},
   "source": [
    "In case you want to train the network by yourself, then the following setting can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f84995-462b-4517-a117-15236a015a2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# epochs = 200\n",
    "# num_batches = 128\n",
    "# batch_size = 32\n",
    "# learning_rate = keras.optimizers.schedules.CosineDecay(5e-4, decay_steps=epochs*num_batches, alpha=1e-6)\n",
    "# optimizer = keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0)\n",
    "\n",
    "\n",
    "# history = workflow.fit_online(\n",
    "#      epochs=epochs,\n",
    "#      batch_size=batch_size,\n",
    "#      num_batches_per_epoch=num_batches\n",
    "#  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b60728-6a4f-4313-8227-651a966fb085",
   "metadata": {},
   "source": [
    "## Robust Estimator\n",
    "We then define a robust estimator, where a small amount of data is assumed to be contaminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e32ab33-62aa-4516-b0b9-2cf75202c4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffusion_experiment_robust(v, a, z, ndt, num_obs, rng=None, *args):\n",
    "    out = np.zeros((num_obs, 2))\n",
    "\n",
    "    #create an array with condition (dummy variable, with two values \"0\" and \"1\")\n",
    "    num_conditions = 2\n",
    "    counts = np.random.multinomial(num_obs, [1/num_conditions] * num_conditions)\n",
    "    condition = np.concatenate([np.full(count, i) for i, count in enumerate(counts)])\n",
    "    np.random.shuffle(condition)\n",
    "    \n",
    "    for n in range(num_obs):\n",
    "        index = condition[n]\n",
    "        rt,resp = diffusion_trial(v[index], a, z, ndt)\n",
    "        out[n, :] = np.array([rt,resp])\n",
    "        \n",
    "    out[:,0] = np.log(out[:,0])\n",
    "\n",
    "    contaminants_rt = np.abs(np.random.standard_t(df=1, size=num_obs))\n",
    "    contaminants_resp = np.random.binomial(n=1,p=0.5,size=num_obs)\n",
    "    \n",
    "    replace = np.random.binomial(n=1, p=.1, size=num_obs)\n",
    "    \n",
    "    out[:,0] = (1-replace)*out[:,0] + (replace)*np.log(contaminants_rt)\n",
    "    out[:,1] = (1-replace)*out[:,1] + (replace)*contaminants_resp\n",
    "    \n",
    "    return dict(rt = out[:,0], resp = out[:,1], con = condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f82f8a-e17f-42d6-917b-d776d3687134",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator_robust = bf.simulators.make_simulator([diffusion_prior, diffusion_experiment_robust], meta_fn=meta)\n",
    "summary_network_robust = bf.networks.SetTransformer(summary_dim=15)\n",
    "inference_network_robust = bf.networks.CouplingFlow(transform=\"spline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43d9d7b-333b-442a-ad3e-ac13c93aea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_robust = bf.BasicWorkflow(\n",
    "    simulator=simulator_robust,\n",
    "    adapter=adapter,\n",
    "    inference_network=inference_network_robust,\n",
    "    summary_network=summary_network_robust,\n",
    "    initial_learning_rate=5e-4,\n",
    "    optimizer = optimizer,\n",
    "    summary_variables = [\"rt\",\"resp\", \"con\"],\n",
    "    inference_variables = [\"drifts\",\"threshold\",\"ndt\",\"z\"],\n",
    "    inference_conditions = [\"num_obs\"],\n",
    "    #checkpoint_filepath = checkpoint_path,\n",
    "    #checkpoint_name = \"robust_ddm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1edc63d-ce9f-476f-a332-860fae3e81f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs_robust = 400\n",
    "# history = workflow_robust.fit_online(\n",
    "#      epochs=epochs_robust,\n",
    "#      batch_size=batch_size,\n",
    "#      num_batches_per_epoch=num_batches\n",
    "#  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1304aac-096f-4b8e-8077-30904b1c3a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path_robust = notebook_path/\"robust_ddm.keras\"\n",
    "workflow_robust.approximator = keras.saving.load_model(checkpoint_path_robust)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
